{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, EncoderDecoderModel, GPT2Tokenizer, BertTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, pipeline\n",
    "from sacrebleu import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a dataframe\n",
    "\n",
    "file_path=\"arawebnlg2020.csv\"\n",
    "categories=['SportsTeam', 'CelestialBody', 'Food', 'Artist', 'University', 'Politician']\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.loc[df['category'].isin(categories)]\n",
    "df = df.dropna()\n",
    "df = df[['input_text_ar', 'target_text_ar', 'data']]\n",
    "print(f\"Found {len(df)} entries.\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, development (validation), and test sets.\n",
    "train_data = Dataset.from_pandas(df[df['data'] == 'train'][['input_text_ar', 'target_text_ar']])\n",
    "dev_data = Dataset.from_pandas(df[df['data'] == 'dev'][['input_text_ar', 'target_text_ar']])\n",
    "test_data = Dataset.from_pandas(df[df['data'] == 'test'][['input_text_ar', 'target_text_ar']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data)\n",
    "print(dev_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "encoder_max_length=256\n",
    "decoder_max_length=256\n",
    "model_name_encoder=\"aubmindlab/bert-base-arabert\"\n",
    "model_name_decoder=\"aubmindlab/aragpt2-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_tokenizer = BertTokenizer.from_pretrained(model_name_encoder)\n",
    "# CLS token will work as BOS token\n",
    "encoder_tokenizer.bos_token = encoder_tokenizer.cls_token\n",
    "# SEP token will work as EOS token\n",
    "encoder_tokenizer.eos_token = encoder_tokenizer.sep_token\n",
    "\n",
    "\n",
    "# make sure GPT2 appends EOS in begin and end\n",
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "    return outputs\n",
    "\n",
    "GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "decoder_tokenizer = GPT2Tokenizer.from_pretrained(model_name_decoder)\n",
    "# set pad_token_id to unk_token_id -> be careful here as unk_token_id == eos_token_id == bos_token_id\n",
    "decoder_tokenizer.pad_token = decoder_tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and process data for model input\n",
    "def process_data_to_model_inputs(batch):\n",
    "    \"\"\"\n",
    "    Tokenize and process the input and target text data for the encoder-decoder model, using separate tokenizers.\n",
    "\n",
    "    Args:\n",
    "        batch (dict): A batch of data with 'input_text_ar' and 'target_text_ar'.\n",
    "        encoder_tokenizer (Tokenizer): The tokenizer to use for the encoder (BERT).\n",
    "        decoder_tokenizer (Tokenizer): The tokenizer to use for the decoder (GPT).\n",
    "        encoder_max_length (int): Maximum token length for the encoder input.\n",
    "        decoder_max_length (int): Maximum token length for the decoder output.\n",
    "\n",
    "    Returns:\n",
    "        dict: A batch with processed input_ids, attention_mask, decoder_input_ids, and labels.\n",
    "    \"\"\"\n",
    "    # Tokenize inputs using BERT tokenizer (for encoder)\n",
    "    inputs = encoder_tokenizer(batch[\"input_text_ar\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "\n",
    "    # Tokenize outputs using GPT tokenizer (for decoder)\n",
    "    outputs = decoder_tokenizer(batch[\"target_text_ar\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "\n",
    "\n",
    "    # Mask padding tokens for loss calculation\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == decoder_tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]\n",
    "    ]\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=['input_text_ar', 'target_text_ar'],\n",
    ")\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "dev_data = dev_data.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=['input_text_ar', 'target_text_ar'],\n",
    ")\n",
    "dev_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "test_data = test_data.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=['input_text_ar', 'target_text_ar'],\n",
    ")\n",
    "test_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = decoder_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = decoder_tokenizer.eos_token_id\n",
    "    label_str = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    return {\"bleu\": round(corpus_bleu(pred_str , [label_str]).score, 4),\n",
    "            \"bleu_1\": round(corpus_bleu(pred_str , [label_str]).precisions[0], 4),\n",
    "            \"bleu_2\": round(corpus_bleu(pred_str , [label_str]).precisions[1], 4),\n",
    "            \"bleu_3\": round(corpus_bleu(pred_str , [label_str]).precisions[2], 4),\n",
    "            \"bleu_4\": round(corpus_bleu(pred_str , [label_str]).precisions[3], 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Encoder-Decoder model based on the specified architecture (BERT2BERT, BERT2GPT, GPT2GPT)\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name_encoder, model_name_decoder, tie_encoder_decoder=False)\n",
    "\n",
    "# set decoding params\n",
    "model.decoder.config.use_cache = False\n",
    "model.config.decoder_start_token_id = decoder_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = decoder_tokenizer.eos_token_id\n",
    "model.config.max_length = 256\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.early_stopping = False\n",
    "model.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = '/output/dir',\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy =\"steps\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    logging_dir = '/log/dir',\n",
    "    save_steps= 1000,\n",
    "    eval_steps=1000,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs = 5,\n",
    "    overwrite_output_dir=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=dev_data,\n",
    "    compute_metrics=compute_metrics # Pass decoder tokenizer for predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "text_generation = pipeline(\"text2text-generation\", model='checkpoint')\n",
    "input_text = ''\n",
    "output_text = text_generation(\n",
    "    input_text,\n",
    "    num_beams=5,\n",
    "    max_length=256,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty = 3.0,\n",
    "    no_repeat_ngram_size = 3\n",
    ")[0]['generated_text']\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
